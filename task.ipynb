{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zahoor-ai-developer/Documents/fall-2024/CV-LAB/i202384_A3/a3_task_updated/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import time\n",
    "\n",
    "# # Path to the video file\n",
    "# video_path = \"HMDB_dataset/jump/testing_1.avi\"\n",
    "\n",
    "# # Open the video file\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# if not cap.isOpened():\n",
    "#     print(\"Error: Could not open video.\")\n",
    "#     exit()\n",
    "\n",
    "# # Calculate FPS using the video file's metadata\n",
    "# fps_metadata = cap.get(cv2.CAP_PROP_FPS)\n",
    "# print(f\"FPS from metadata: {fps_metadata}\")\n",
    "\n",
    "# # Alternatively, calculate FPS dynamically by measuring the time to process frames\n",
    "# frame_count = 0\n",
    "# start_time = time.time()\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     frame_count += 1\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(frame_count)\n",
    "# # Calculate FPS dynamically\n",
    "# duration = end_time - start_time\n",
    "# fps_dynamic = frame_count / duration if duration > 0 else 0\n",
    "\n",
    "# print(f\"FPS calculated dynamically: {fps_dynamic:.2f}\")\n",
    "\n",
    "# # Release the video capture object\n",
    "# cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def extract_frames_from_videos(\n",
    "#     dataset_path,\n",
    "#     output_path,\n",
    "#     stride=10,\n",
    "#     video_extensions=(\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Extracts every nth frame from videos in the dataset and organizes them into class-specific folders.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset_path (str): Path to the HMDB_dataset directory.\n",
    "#     - output_path (str): Path where the Frames_dataset will be saved.\n",
    "#     - stride (int): Interval of frames to extract (e.g., every 10th frame).\n",
    "#     - video_extensions (tuple): Accepted video file extensions.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Ensure the output directory exists\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "#     # List all class directories in the dataset\n",
    "#     classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    \n",
    "#     if not classes:\n",
    "#         print(f\"No class directories found in {dataset_path}. Please check the dataset path.\")\n",
    "#         return\n",
    "    \n",
    "#     for class_name in classes:\n",
    "#         class_input_path = os.path.join(dataset_path, class_name)\n",
    "#         class_output_path = os.path.join(output_path, class_name)\n",
    "        \n",
    "#         # Create the class output directory if it doesn't exist\n",
    "#         os.makedirs(class_output_path, exist_ok=True)\n",
    "        \n",
    "#         # List all video files in the class directory\n",
    "#         videos = [\n",
    "#             f for f in os.listdir(class_input_path)\n",
    "#             if os.path.isfile(os.path.join(class_input_path, f)) and f.lower().endswith(video_extensions)\n",
    "#         ]\n",
    "        \n",
    "#         if not videos:\n",
    "#             print(f\"No video files found in {class_input_path}. Skipping this class.\")\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"\\nProcessing Class: {class_name} | Total Videos: {len(videos)}\")\n",
    "        \n",
    "#         for video_file in tqdm(videos, desc=f\"Extracting from {class_name}\", unit=\"video\"):\n",
    "#             video_path = os.path.join(class_input_path, video_file)\n",
    "#             video_name, _ = os.path.splitext(video_file)\n",
    "            \n",
    "#             # Initialize video capture\n",
    "#             cap = cv2.VideoCapture(video_path)\n",
    "            \n",
    "#             if not cap.isOpened():\n",
    "#                 print(f\"Error: Could not open video file {video_path}. Skipping.\")\n",
    "#                 continue\n",
    "            \n",
    "#             frame_count = 0\n",
    "#             saved_frames = 0\n",
    "            \n",
    "#             while True:\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret:\n",
    "#                     break\n",
    "                \n",
    "#                 if frame_count % stride == 0:\n",
    "#                     frame_filename = f\"{video_name}_frame{frame_count:05d}.jpg\"\n",
    "#                     frame_path = os.path.join(class_output_path, frame_filename)\n",
    "                    \n",
    "#                     # Save the frame as JPEG\n",
    "#                     success = cv2.imwrite(frame_path, frame)\n",
    "#                     if success:\n",
    "#                         saved_frames += 1\n",
    "#                     else:\n",
    "#                         print(f\"Warning: Failed to save frame {frame_filename}.\")\n",
    "                \n",
    "#                 frame_count += 1\n",
    "            \n",
    "#             cap.release()\n",
    "#             # Optional: Print summary for each video\n",
    "#             # print(f\"Extracted {saved_frames} frames from {video_file}\")\n",
    "\n",
    "#     print(\"\\nFrame extraction completed successfully.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define input and output paths\n",
    "#     dataset_path = \"HMDB_dataset\"        # Replace with your actual dataset path\n",
    "#     output_path = \"Frames_dataset\"       # Replace with your desired output path\n",
    "    \n",
    "#     # Set the stride (extract every 10th frame)\n",
    "#     stride = 10\n",
    "    \n",
    "#     # Start frame extraction\n",
    "#     extract_frames_from_videos(\n",
    "#         dataset_path=dataset_path,\n",
    "#         output_path=output_path,\n",
    "#         stride=stride\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import random\n",
    "# import shutil\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def split_frames_into_train_test(\n",
    "#     input_dir,\n",
    "#     output_dir,\n",
    "#     train_ratio=0.8,\n",
    "#     resize_dim=(224, 224),\n",
    "#     seed=42,\n",
    "#     image_extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Splits frames into training and testing sets and resizes them.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_dir (str): Path to the original frames directory.\n",
    "#     - output_dir (str): Path where the split and resized frames will be saved.\n",
    "#     - train_ratio (float): Proportion of frames to include in the training set.\n",
    "#     - resize_dim (tuple): Desired size for the frames (width, height).\n",
    "#     - seed (int): Seed for random operations to ensure reproducibility.\n",
    "#     - image_extensions (tuple): Accepted image file extensions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Set the random seed for reproducibility\n",
    "#     random.seed(seed)\n",
    "\n",
    "#     # Define training and testing directories\n",
    "#     train_dir = os.path.join(output_dir, \"training\")\n",
    "#     test_dir = os.path.join(output_dir, \"testing\")\n",
    "\n",
    "#     # Create the training and testing directories if they don't exist\n",
    "#     os.makedirs(train_dir, exist_ok=True)\n",
    "#     os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "#     # List all class directories in the input directory\n",
    "#     classes = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]\n",
    "\n",
    "#     if not classes:\n",
    "#         print(f\"No class directories found in {input_dir}. Please check the input directory.\")\n",
    "#         return\n",
    "\n",
    "#     for class_name in classes:\n",
    "#         print(f\"\\nProcessing Class: {class_name}\")\n",
    "\n",
    "#         # Define paths for the current class\n",
    "#         class_input_path = os.path.join(input_dir, class_name)\n",
    "#         class_train_path = os.path.join(train_dir, class_name)\n",
    "#         class_test_path = os.path.join(test_dir, class_name)\n",
    "\n",
    "#         # Create class directories in training and testing folders\n",
    "#         os.makedirs(class_train_path, exist_ok=True)\n",
    "#         os.makedirs(class_test_path, exist_ok=True)\n",
    "\n",
    "#         # List all image files in the class directory\n",
    "#         frames = [\n",
    "#             f for f in os.listdir(class_input_path)\n",
    "#             if os.path.isfile(os.path.join(class_input_path, f)) and f.lower().endswith(image_extensions)\n",
    "#         ]\n",
    "\n",
    "#         if not frames:\n",
    "#             print(f\"No image files found in {class_input_path}. Skipping this class.\")\n",
    "#             continue\n",
    "\n",
    "#         # Shuffle frames to ensure random splitting\n",
    "#         random.shuffle(frames)\n",
    "\n",
    "#         # Calculate split index\n",
    "#         split_idx = int(len(frames) * train_ratio)\n",
    "\n",
    "#         # Split frames into training and testing sets\n",
    "#         train_frames = frames[:split_idx]\n",
    "#         test_frames = frames[split_idx:]\n",
    "\n",
    "#         print(f\"Total Frames: {len(frames)} | Training: {len(train_frames)} | Testing: {len(test_frames)}\")\n",
    "\n",
    "#         # Process Training Frames\n",
    "#         print(f\"Resizing and copying Training frames for class '{class_name}'...\")\n",
    "#         for frame in tqdm(train_frames, desc=f\"Training | {class_name}\", unit=\"frame\"):\n",
    "#             src_path = os.path.join(class_input_path, frame)\n",
    "#             dst_path = os.path.join(class_train_path, frame)\n",
    "\n",
    "#             # Read the image\n",
    "#             image = cv2.imread(src_path)\n",
    "#             if image is None:\n",
    "#                 print(f\"Warning: Failed to read {src_path}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Resize the image\n",
    "#             resized_image = cv2.resize(image, resize_dim)\n",
    "\n",
    "#             # Save the resized image\n",
    "#             success = cv2.imwrite(dst_path, resized_image)\n",
    "#             if not success:\n",
    "#                 print(f\"Warning: Failed to write {dst_path}.\")\n",
    "\n",
    "#         # Process Testing Frames\n",
    "#         print(f\"Resizing and copying Testing frames for class '{class_name}'...\")\n",
    "#         for frame in tqdm(test_frames, desc=f\"Testing | {class_name}\", unit=\"frame\"):\n",
    "#             src_path = os.path.join(class_input_path, frame)\n",
    "#             dst_path = os.path.join(class_test_path, frame)\n",
    "\n",
    "#             # Read the image\n",
    "#             image = cv2.imread(src_path)\n",
    "#             if image is None:\n",
    "#                 print(f\"Warning: Failed to read {src_path}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Resize the image\n",
    "#             resized_image = cv2.resize(image, resize_dim)\n",
    "\n",
    "#             # Save the resized image\n",
    "#             success = cv2.imwrite(dst_path, resized_image)\n",
    "#             if not success:\n",
    "#                 print(f\"Warning: Failed to write {dst_path}.\")\n",
    "\n",
    "#     print(\"\\nFrame splitting and resizing completed successfully.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define input and output paths\n",
    "#     input_frames_dir = \"Frames_dataset\"    # Replace with your actual frames directory\n",
    "#     output_frames_dir = \"Frames_dataset_split\"      # Replace with your desired output directory\n",
    "\n",
    "#     # Parameters\n",
    "#     training_ratio = 0.8                            # 80% training, 20% testing\n",
    "#     resize_dimensions = (224, 224)                   # Resize frames to 224x224 pixels\n",
    "#     random_seed = 42                                 # Seed for reproducibility\n",
    "\n",
    "#     # Start the splitting and resizing process\n",
    "#     split_frames_into_train_test(\n",
    "#         input_dir=input_frames_dir,\n",
    "#         output_dir=output_frames_dir,\n",
    "#         train_ratio=training_ratio,\n",
    "#         resize_dim=resize_dimensions,\n",
    "#         seed=random_seed\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Available: True\n",
      "CUDA Version (PyTorch): 12.4\n",
      "cuDNN Version: 90100\n",
      "GPU Name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version (PyTorch):\", torch.version.cuda)\n",
    "print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread(\"Frames_dataset_split/testing/jump/testing_2_frame00030.jpg\")\n",
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zahoor-ai-developer/Documents/fall-2024/CV-LAB/i202384_A3/a3_task_updated/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Configuration and Setup\n",
    "# ==========================\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to the training and testing directories\n",
    "train_dir = \"Frames_dataset_split/training\"  # Replace with your training frames directory\n",
    "test_dir = \"Frames_dataset_split/testing\"    # Replace with your testing frames directory\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 51          # Number of target classes\n",
    "learning_rate = 1e-4     # Learning rate for optimizer\n",
    "num_epochs = 5          # Number of training epochs\n",
    "batch_size = 32          # Batch size for DataLoader\n",
    "stride = 10              # Stride used during frame extraction (if applicable)\n",
    "\n",
    "# Initialize the feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['brush_hair', 'cartwheel', 'catch', 'chew', 'clap', 'climb', 'climb_stairs', 'dive', 'draw_sword', 'dribble', 'drink', 'eat', 'fall_floor', 'fencing', 'flic_flac', 'golf', 'handstand', 'hit', 'hug', 'jump', 'kick', 'kick_ball', 'kiss', 'laugh', 'pick', 'pour', 'pullup', 'punch', 'push', 'pushup', 'ride_bike', 'ride_horse', 'run', 'shake_hands', 'shoot_ball', 'shoot_bow', 'shoot_gun', 'sit', 'situp', 'smile', 'smoke', 'somersault', 'stand', 'swing_baseball', 'sword', 'sword_exercise', 'talk', 'throw', 'turn', 'walk', 'wave']\n"
     ]
    }
   ],
   "source": [
    "# Define image transformations using the feature extractor\n",
    "def get_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),               # Resize images to 224x224\n",
    "        transforms.ToTensor(),                       # Convert PIL Image to Tensor\n",
    "        transforms.Normalize(mean=feature_extractor.image_mean,\n",
    "                             std=feature_extractor.image_std),  # Normalize based on feature extractor\n",
    "    ])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=get_transforms())\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=get_transforms())\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Class names\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([51]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTSdpaAttention(\n",
      "            (attention): ViTSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=51, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Model Setup\n",
    "# ==========================\n",
    "\n",
    "# Load the pretrained ViT model with num_labels=51\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=num_classes,              # Set to 51\n",
    "    ignore_mismatched_sizes=True        # Ignore size mismatch for the classifier head\n",
    ")\n",
    "\n",
    "# Freeze all model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classifier head parameters\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move the model to the configured device\n",
    "model.to(device)\n",
    "\n",
    "# Print model architecture (optional)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Loss Function and Optimizer\n",
    "# ==========================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only parameters of the classifier head are being optimized\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Training and Evaluation Functions\n",
    "# ==========================\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1647 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   Loss: 1.8250 | Accuracy: 56.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing    Loss: 1.2402 | Accuracy: 67.98%\n",
      "Best model updated with accuracy: 67.98%\n",
      "\n",
      "Epoch 2/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   Loss: 1.0686 | Accuracy: 72.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing    Loss: 0.9928 | Accuracy: 73.77%\n",
      "Best model updated with accuracy: 73.77%\n",
      "\n",
      "Epoch 3/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   Loss: 0.8814 | Accuracy: 76.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing    Loss: 0.8735 | Accuracy: 76.78%\n",
      "Best model updated with accuracy: 76.78%\n",
      "\n",
      "Epoch 4/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   Loss: 0.7744 | Accuracy: 79.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing    Loss: 0.7997 | Accuracy: 78.94%\n",
      "Best model updated with accuracy: 78.94%\n",
      "\n",
      "Epoch 5/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   Loss: 0.7017 | Accuracy: 81.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing    Loss: 0.7473 | Accuracy: 79.89%\n",
      "Best model updated with accuracy: 79.89%\n",
      "\n",
      "Training completed. Best Test Accuracy: 79.89%\n",
      "Best model saved to best_vit_model.pth\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Main Training Loop\n",
    "# ==========================\n",
    "\n",
    "best_test_acc = 0.0\n",
    "best_model_path = \"best_vit_model.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Training   Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Testing    Loss: {test_loss:.4f} | Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Save the model if it has the best accuracy so far\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Best model updated with accuracy: {best_test_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining completed. Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "print(f\"Best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Load the Best Model for Evaluation\n",
    "# ==========================\n",
    "\n",
    "# To load the best model for future use:\n",
    "# model.load_state_dict(torch.load(best_model_path))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# ==========================\n",
    "# Example Inference\n",
    "# ==========================\n",
    "\n",
    "def predict(model, image_path, feature_extractor, device, class_names):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "    \n",
    "    print(f\"Predicted Class: {class_names[predicted_class.item()]} | Confidence: {confidence.item():.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "# predict(model, \"path_to_sample_image.jpg\", feature_extractor, device, class_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3_task_updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
